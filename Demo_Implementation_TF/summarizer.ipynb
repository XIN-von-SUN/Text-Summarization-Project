{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bit8d437ac45ab1400083d7026870debc43",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/Users/xinsun/Dev_env/Text-Summarization-Project/Implementation1\n1.14.0\n[nltk_data] Downloading package punkt to /Users/xinsun/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_hub as hub\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import data_process\n",
    "import model\n",
    "\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Reviews.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(568454, 10)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Id   ProductId          UserId ProfileName  HelpfulnessNumerator  \\\n0   1  B001E4KFG0  A3SGXH7AUHU8GW  delmartian                     1   \n\n   HelpfulnessDenominator  Score        Time                Summary  \\\n0                       1      5  1303862400  Good Quality Dog Food   \n\n                                                Text  \n0  I have bought several of the Vitality canned d...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Id                         0\nProductId                  0\nUserId                     0\nProfileName               16\nHelpfulnessNumerator       0\nHelpfulnessDenominator     0\nScore                      0\nTime                       0\nSummary                   27\nText                       0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 Summary                                               Text\n0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "data.dropna(subset=['Summary'],inplace = True)\n",
    "data = data[['Summary', 'Text']]\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = []\n",
    "raw_summaries = []\n",
    "\n",
    "for text, summary in zip(data.Text, data.Summary):\n",
    "    if 20 < len(text) < 300:\n",
    "        raw_texts.append(text)\n",
    "        raw_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(568427, 2)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(282345, 282345)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "print(data.shape)\n",
    "len(raw_texts), len(raw_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Processing Time:  201.13380217552185\n"
    }
   ],
   "source": [
    "# the function gives us the option to keep_most of the characters inisde the texts and summaries, meaning\n",
    "# punctuation, question marks, slashes...\n",
    "# or we can set it to False, meaning we only want to keep letters and numbers like here.\n",
    "processed_texts, processed_summaries, words_counted = data_process.preprocess_texts_and_summaries(\n",
    "            raw_texts,\n",
    "            raw_summaries,\n",
    "            keep_most=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text\n: ['i', 'have', 'bought', 'several', 'of', 'the', 'vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', 'the', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', 'my', 'labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', 'most'] \n\nSummary:\n ['good', 'quality', 'dog', 'food'] \n\n\n\n"
    }
   ],
   "source": [
    "for t,s in zip(processed_texts[:1], processed_summaries[:1]):\n",
    "    print('Text\\n:', t, '\\n')\n",
    "    print('Summary:\\n', s, '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create lookup dicts\n",
    "\n",
    "We cannot feed our network actual words, but numbers. So we first have to create our lookup dicts, where each words gets and int value (high or low, depending on its frequency in our corpus). Those help us to later convert the texts into numbers.\n",
    "\n",
    "We also add special tokens. EndOfSentence and StartOfSentence are crucial for the Seq2Seq model we later use.\n",
    "Pad token, because all summaries and texts in a batch need to have the same length, pad token helps us do that.\n",
    "\n",
    "So we need 2 lookup dicts:\n",
    " - From word to index \n",
    " - from index to word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "57160 57160 0\n"
    }
   ],
   "source": [
    "specials = [\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"]\n",
    "word2ind, ind2word,  ignore_words = data_process.create_word_indx_dicts(words_counted, specials=specials)\n",
    "\n",
    "print(len(word2ind), len(ind2word), len(ignore_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-6b8aed387369>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-6b8aed387369>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Optionally we can use pretrained word embeddings. Those have proved to increase training speed and accuracy.\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Pretrained embeddings\n",
    "\n",
    "Optionally we can use pretrained word embeddings. Those have proved to increase training speed and accuracy.\n",
    "Here I used two different options. Either we use glove embeddings or embeddings from tf_hub.\n",
    "The ones from tf_hub worked better, so we use those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'<SOS>': 0,\n '<EOS>': 1,\n '<PAD>': 2,\n '<UNK>': 3,\n 'the': 4,\n 'i': 5,\n 'and': 6,\n 'a': 7,\n 'it': 8,\n 'this': 9,\n 'to': 10,\n 'is': 11,\n 'for': 12,\n 'of': 13,\n 'my': 14,\n 'in': 15,\n 'great': 16,\n 'good': 17,\n 'not': 18,\n 'but': 19,\n 'are': 20,\n 'these': 21,\n 'have': 22,\n 'they': 23,\n 'you': 24,\n 'with': 25,\n 'was': 26,\n 'that': 27,\n 'love': 28,\n 's': 29,\n 'like': 30,\n 't': 31,\n 'coffee': 32,\n 'them': 33,\n 'very': 34,\n 'as': 35,\n 'on': 36,\n 'product': 37,\n 'so': 38,\n 'taste': 39,\n 'tea': 40,\n 'flavor': 41,\n 'best': 42,\n 'can': 43,\n 'just': 44,\n 'at': 45,\n 'one': 46,\n 'all': 47,\n 'be': 48,\n 'we': 49,\n 'will': 50,\n 'price': 51,\n 'or': 52,\n 'if': 53,\n 'too': 54,\n 'has': 55,\n 'from': 56,\n 'more': 57,\n 'really': 58,\n 'than': 59,\n 'buy': 60,\n 'would': 61,\n 'when': 62,\n 'me': 63,\n 'had': 64,\n 'amazon': 65,\n 'no': 66,\n 'delicious': 67,\n 'get': 68,\n 'food': 69,\n 'much': 70,\n 'dog': 71,\n 'only': 72,\n 'find': 73,\n 'better': 74,\n 'out': 75,\n 'time': 76,\n 'use': 77,\n 'little': 78,\n 'other': 79,\n 'loves': 80,\n 'again': 81,\n 'were': 82,\n 'favorite': 83,\n 'up': 84,\n 'what': 85,\n 'don': 86,\n 've': 87,\n 'some': 88,\n 'tried': 89,\n 'cup': 90,\n 'well': 91,\n 'your': 92,\n 'tastes': 93,\n 'chocolate': 94,\n 'am': 95,\n 'an': 96,\n 'eat': 97,\n 'excellent': 98,\n 'make': 99,\n 'free': 100,\n 'about': 101,\n 'our': 102,\n 'order': 103,\n 'been': 104,\n 'also': 105,\n 'do': 106,\n 'nice': 107,\n 'there': 108,\n 'ever': 109,\n 'sweet': 110,\n 'snack': 111,\n 'he': 112,\n 'even': 113,\n 'bought': 114,\n 'tasty': 115,\n 'now': 116,\n 'm': 117,\n 'perfect': 118,\n 'by': 119,\n 'try': 120,\n 'stuff': 121,\n 'any': 122,\n 'because': 123,\n 'she': 124,\n 'recommend': 125,\n 'dogs': 126,\n 'found': 127,\n 'sugar': 128,\n 'chips': 129,\n 'after': 130,\n 'easy': 131,\n 'healthy': 132,\n 'k': 133,\n 'way': 134,\n 'store': 135,\n 'drink': 136,\n 'used': 137,\n 'treats': 138,\n 'hot': 139,\n 'wonderful': 140,\n 'go': 141,\n 'quality': 142,\n 'bag': 143,\n '2': 144,\n 'yummy': 145,\n 'strong': 146,\n 'box': 147,\n 'treat': 148,\n 'day': 149,\n 'made': 150,\n 'mix': 151,\n 'fresh': 152,\n 'makes': 153,\n 'got': 154,\n 'tasting': 155,\n 'cookies': 156,\n 'ordered': 157,\n 'which': 158,\n 'hard': 159,\n 'every': 160,\n 'always': 161,\n 'over': 162,\n 'right': 163,\n 'its': 164,\n 'happy': 165,\n 'who': 166,\n 'flavors': 167,\n 'first': 168,\n 'cups': 169,\n 'many': 170,\n 'could': 171,\n 'their': 172,\n 'her': 173,\n 'without': 174,\n 'salt': 175,\n 'never': 176,\n 'brand': 177,\n 'does': 178,\n 'enjoy': 179,\n 'think': 180,\n 'water': 181,\n 'gluten': 182,\n 'still': 183,\n 'bad': 184,\n 'most': 185,\n 'purchase': 186,\n 'bit': 187,\n 'buying': 188,\n 'two': 189,\n 'highly': 190,\n 're': 191,\n 'green': 192,\n '3': 193,\n 'old': 194,\n 'stores': 195,\n 'definitely': 196,\n 'shipping': 197,\n 'cat': 198,\n 'loved': 199,\n 'organic': 200,\n 'did': 201,\n 'years': 202,\n 'keep': 203,\n 'bars': 204,\n 'lot': 205,\n 'know': 206,\n 'since': 207,\n 'far': 208,\n 'pack': 209,\n 'give': 210,\n 'then': 211,\n 'candy': 212,\n 'size': 213,\n 'enough': 214,\n 'awesome': 215,\n 'small': 216,\n '5': 217,\n 'regular': 218,\n 'before': 219,\n '1': 220,\n 'want': 221,\n 'worth': 222,\n 'add': 223,\n 'need': 224,\n 'arrived': 225,\n 'less': 226,\n 'milk': 227,\n 'doesn': 228,\n 'tasted': 229,\n 'local': 230,\n 'different': 231,\n 'long': 232,\n 'bags': 233,\n 'fast': 234,\n 'cats': 235,\n 'how': 236,\n 'low': 237,\n 'same': 238,\n 'here': 239,\n 'say': 240,\n 'pretty': 241,\n 'expensive': 242,\n 'll': 243,\n 'item': 244,\n 'didn': 245,\n 'whole': 246,\n 'high': 247,\n 'kids': 248,\n 'purchased': 249,\n 'morning': 250,\n 'butter': 251,\n 'eating': 252,\n 'work': 253,\n 'few': 254,\n 'products': 255,\n 'back': 256,\n 'real': 257,\n 'deal': 258,\n 'big': 259,\n 'sauce': 260,\n 'looking': 261,\n 'sure': 262,\n 'received': 263,\n 'smooth': 264,\n 'family': 265,\n 'something': 266,\n '4': 267,\n 'oil': 268,\n 'blend': 269,\n 'though': 270,\n 'dark': 271,\n 'money': 272,\n 'cereal': 273,\n 'popcorn': 274,\n 'gift': 275,\n 'coconut': 276,\n 'husband': 277,\n 'using': 278,\n 'however': 279,\n 'grocery': 280,\n 'thought': 281,\n 'while': 282,\n 'came': 283,\n 'amazing': 284,\n 'value': 285,\n 'full': 286,\n 'his': 287,\n 'both': 288,\n 'yum': 289,\n 'wish': 290,\n 'absolutely': 291,\n 'year': 292,\n 'bold': 293,\n 'bitter': 294,\n 'last': 295,\n 'put': 296,\n 'works': 297,\n 'natural': 298,\n 'package': 299,\n 'thing': 300,\n 'bar': 301,\n 'vanilla': 302,\n 'chicken': 303,\n 'disappointed': 304,\n 'texture': 305,\n 'new': 306,\n 'flavored': 307,\n 'into': 308,\n 'breakfast': 309,\n 'protein': 310,\n 'quick': 311,\n 'take': 312,\n 'off': 313,\n 'rice': 314,\n 'almost': 315,\n 'kind': 316,\n 'anything': 317,\n 'quite': 318,\n 'cheaper': 319,\n 'down': 320,\n 'light': 321,\n 'fruit': 322,\n 'through': 323,\n 'ingredients': 324,\n 'peanut': 325,\n 'being': 326,\n 'diet': 327,\n 'won': 328,\n 'available': 329,\n 'decaf': 330,\n 'feel': 331,\n 'quickly': 332,\n 'thanks': 333,\n 'each': 334,\n 'ordering': 335,\n 'around': 336,\n 'should': 337,\n 'brands': 338,\n 'glad': 339,\n 'going': 340,\n 'rich': 341,\n 'home': 342,\n 'those': 343,\n 'calories': 344,\n 'smell': 345,\n 'another': 346,\n 'fat': 347,\n 'roast': 348,\n 'actually': 349,\n 'keurig': 350,\n 'amount': 351,\n '6': 352,\n 'black': 353,\n 'having': 354,\n 'son': 355,\n 'several': 356,\n 'ok': 357,\n 'cheese': 358,\n 'getting': 359,\n 'service': 360,\n 'beans': 361,\n 'away': 362,\n 'jerky': 363,\n 'thank': 364,\n 'dry': 365,\n 'able': 366,\n 'nothing': 367,\n 'see': 368,\n 'delivery': 369,\n 'd': 370,\n 'variety': 371,\n 'especially': 372,\n 'honey': 373,\n 'save': 374,\n 'half': 375,\n 'spicy': 376,\n 'likes': 377,\n 'people': 378,\n 'him': 379,\n 'bread': 380,\n 'fantastic': 381,\n 'ones': 382,\n 'come': 383,\n 'house': 384,\n 'baby': 385,\n 'super': 386,\n 'recommended': 387,\n 'soup': 388,\n 'salty': 389,\n 'seems': 390,\n 'packaging': 391,\n 'liked': 392,\n 'why': 393,\n 'company': 394,\n 'large': 395,\n 'syrup': 396,\n 'us': 397,\n 'trying': 398,\n 'daughter': 399,\n 'once': 400,\n 'pleased': 401,\n 'must': 402,\n 'case': 403,\n 'crunchy': 404,\n 'foods': 405,\n 'expected': 406,\n 'boxes': 407,\n 'where': 408,\n 'extra': 409,\n 'usually': 410,\n 'days': 411,\n 'own': 412,\n 'per': 413,\n 'making': 414,\n 'everyone': 415,\n 'fine': 416,\n 'others': 417,\n 'yet': 418,\n '10': 419,\n 'anyone': 420,\n 'added': 421,\n 'teas': 422,\n 'snacks': 423,\n 'prefer': 424,\n 'cream': 425,\n 'things': 426,\n 'enjoyed': 427,\n 'drinking': 428,\n 'cookie': 429,\n 'pasta': 430,\n 'cost': 431,\n 'comes': 432,\n 'problem': 433,\n 'everything': 434,\n 'flavorful': 435,\n 'soft': 436,\n '12': 437,\n 'said': 438,\n 'nuts': 439,\n 'months': 440,\n 'com': 441,\n 'health': 442,\n 'plus': 443,\n 'times': 444,\n 'look': 445,\n 'white': 446,\n 'month': 447,\n 'instead': 448,\n 'such': 449,\n 'energy': 450,\n 'three': 451,\n 'wanted': 452,\n 'else': 453,\n 'longer': 454,\n 'probably': 455,\n 'convenient': 456,\n 'market': 457,\n 'top': 458,\n 'gave': 459,\n 'gum': 460,\n 'teeth': 461,\n 'may': 462,\n 'wife': 463,\n 'alternative': 464,\n 'isn': 465,\n 'online': 466,\n 'bottle': 467,\n 'friends': 468,\n 'fan': 469,\n 'spice': 470,\n 'beef': 471,\n 'cinnamon': 472,\n 'exactly': 473,\n 'meal': 474,\n 'french': 475,\n 'crackers': 476,\n 'cocoa': 477,\n 'oz': 478,\n 'seem': 479,\n 'help': 480,\n 'couldn': 481,\n 'starbucks': 482,\n 'ginger': 483,\n 'ice': 484,\n 'chai': 485,\n 'smells': 486,\n 'weak': 487,\n 'next': 488,\n 'oatmeal': 489,\n 'red': 490,\n 'review': 491,\n 'goes': 492,\n 'stars': 493,\n 'cold': 494,\n 'coffees': 495,\n 'corn': 496,\n 'pieces': 497,\n 'fact': 498,\n 'maybe': 499,\n 'brew': 500,\n 'cans': 501,\n 'lemon': 502,\n 'aroma': 503,\n 'chew': 504,\n 'wheat': 505,\n 'potato': 506,\n 'least': 507,\n 'reviews': 508,\n 'picky': 509,\n 'mild': 510,\n 'powder': 511,\n '8': 512,\n 'shipped': 513,\n 'wrong': 514,\n 'until': 515,\n 'juice': 516,\n 'bulk': 517,\n 'choice': 518,\n 'packaged': 519,\n 'says': 520,\n 'dried': 521,\n 'chip': 522,\n 'iced': 523,\n 'stale': 524,\n 'delivered': 525,\n 'instant': 526,\n 'friend': 527,\n 'christmas': 528,\n 'cooking': 529,\n 'either': 530,\n 'weight': 531,\n 'couple': 532,\n 'went': 533,\n 'myself': 534,\n 'gets': 535,\n 'almonds': 536,\n 'night': 537,\n 'packs': 538,\n 'mouth': 539,\n 'cake': 540,\n 'special': 541,\n 'machine': 542,\n 'mountain': 543,\n 'pop': 544,\n 'date': 545,\n 'clean': 546,\n 'sent': 547,\n 'although': 548,\n 'helps': 549,\n 'refreshing': 550,\n 'plain': 551,\n 'open': 552,\n 'subscribe': 553,\n 'ago': 554,\n 'smaller': 555,\n 'lots': 556,\n 'second': 557,\n 'apple': 558,\n 'haven': 559,\n 'wow': 560,\n 'waste': 561,\n 'meat': 562,\n 'might': 563,\n 'training': 564,\n 'week': 565,\n 'took': 566,\n 'wasn': 567,\n 'chewy': 568,\n 'hazelnut': 569,\n 'live': 570,\n '100': 571,\n 'espresso': 572,\n 'simply': 573,\n 'licorice': 574,\n 'run': 575,\n 'pods': 576,\n 'seeds': 577,\n 'continue': 578,\n 'life': 579,\n 'seasoning': 580,\n 'crunch': 581,\n 'needed': 582,\n 'close': 583,\n 'ounce': 584,\n '50': 585,\n 'pretzels': 586,\n 'sometimes': 587,\n 'cook': 588,\n 'believe': 589,\n 'filling': 590,\n 'pay': 591,\n 'pricey': 592,\n 'place': 593,\n 'gives': 594,\n 'original': 595,\n 'almond': 596,\n 'seller': 597,\n 'aftertaste': 598,\n 'care': 599,\n 'stock': 600,\n 'surprised': 601,\n 'medium': 602,\n 'wouldn': 603,\n 'tell': 604,\n 'problems': 605,\n 'keeps': 606,\n 'mixed': 607,\n 'rather': 608,\n 'start': 609,\n 'please': 610,\n 'line': 611,\n 'version': 612,\n 'beat': 613,\n 'side': 614,\n 'type': 615,\n 'difference': 616,\n 'soy': 617,\n 'flour': 618,\n 'giving': 619,\n 'stash': 620,\n 'artificial': 621,\n 'hand': 622,\n 'calorie': 623,\n 'plastic': 624,\n 'hit': 625,\n 'terrible': 626,\n 'canned': 627,\n 'easily': 628,\n 'crazy': 629,\n 'maker': 630,\n 'started': 631,\n 'lunch': 632,\n 'locally': 633,\n 'minutes': 634,\n 'condition': 635,\n 'fun': 636,\n 'extremely': 637,\n 'compared': 638,\n 'okay': 639,\n 'looks': 640,\n 'gone': 641,\n 'priced': 642,\n 'container': 643,\n 'hope': 644,\n 'soon': 645,\n 'fiber': 646,\n 'puppy': 647,\n 'etc': 648,\n 'reasonable': 649,\n 'often': 650,\n 'needs': 651,\n 'horrible': 652,\n 'granola': 653,\n 'wait': 654,\n 'blue': 655,\n 'recipe': 656,\n 'let': 657,\n 'carry': 658,\n 'drinks': 659,\n 'huge': 660,\n 'chews': 661,\n 'orange': 662,\n 'mint': 663,\n 'brown': 664,\n 'read': 665,\n 'baking': 666,\n 'pepper': 667,\n 'finally': 668,\n 'favorites': 669,\n 'anywhere': 670,\n 'non': 671,\n 'decent': 672,\n 'cut': 673,\n 'satisfied': 674,\n 'broken': 675,\n 'caffeine': 676,\n 'color': 677,\n 'awful': 678,\n 'sour': 679,\n 'guess': 680,\n 'stomach': 681,\n 'vinegar': 682,\n '20': 683,\n 'greenies': 684,\n 'creamy': 685,\n 'packed': 686,\n 'ate': 687,\n 'eaten': 688,\n 'substitute': 689,\n 'sale': 690,\n 'salad': 691,\n 'bones': 692,\n 'packages': 693,\n 'part': 694,\n 'takes': 695,\n 'during': 696,\n 'dont': 697,\n 'jar': 698,\n 'reason': 699,\n 'stop': 700,\n 'world': 701,\n 'larger': 702,\n 'shipment': 703,\n 'bite': 704,\n 'past': 705,\n 'cheap': 706,\n 'purchasing': 707,\n 'soda': 708,\n 'already': 709,\n 'decided': 710,\n 'nut': 711,\n 'noodles': 712,\n 'opened': 713,\n 'daily': 714,\n 'fish': 715,\n 'idea': 716,\n 'healthier': 717,\n 'eats': 718,\n 'hooked': 719,\n '24': 720,\n 'stick': 721,\n 'satisfying': 722,\n 'weeks': 723,\n 'o': 724,\n 'olive': 725,\n 'slightly': 726,\n 'heat': 727,\n 'anymore': 728,\n 'serving': 729,\n 'pet': 730,\n 'fabulous': 731,\n 'single': 732,\n 'change': 733,\n '7': 734,\n 'items': 735,\n 'between': 736,\n 'name': 737,\n 'grain': 738,\n 'pumpkin': 739,\n 'overall': 740,\n 'totally': 741,\n 'bottles': 742,\n 'sold': 743,\n 'baked': 744,\n 'star': 745,\n 'easier': 746,\n 'shop': 747,\n 'count': 748,\n 'grey': 749,\n 'four': 750,\n 'sodium': 751,\n 'excited': 752,\n 'pound': 753,\n 'formula': 754,\n 'similar': 755,\n 'mom': 756,\n 'dinner': 757,\n 'chili': 758,\n '00': 759,\n 'bland': 760,\n 'sticks': 761,\n 'raw': 762,\n 'hint': 763,\n 'customer': 764,\n 'mine': 765,\n 'addictive': 766,\n 'twice': 767,\n 'prices': 768,\n 'pure': 769,\n 'area': 770,\n 'china': 771,\n 'finding': 772,\n 'bbq': 773,\n 'earl': 774,\n 'option': 775,\n 'oh': 776,\n 'lover': 777,\n 'end': 778,\n 'itself': 779,\n 'someone': 780,\n 'everyday': 781,\n 'along': 782,\n 'truly': 783,\n 'kick': 784,\n 'office': 785,\n 'difficult': 786,\n 'pick': 787,\n 'true': 788,\n 'bean': 789,\n 'salmon': 790,\n 'looked': 791,\n 'worked': 792,\n 'feed': 793,\n 'carb': 794,\n 'perfectly': 795,\n 'italian': 796,\n '99': 797,\n 'yes': 798,\n 'body': 799,\n 'unfortunately': 800,\n 'addicted': 801,\n 'senseo': 802,\n 'break': 803,\n 'spices': 804,\n 'left': 805,\n 'cherry': 806,\n 'caramel': 807,\n 'stronger': 808,\n 'ground': 809,\n 'combination': 810,\n 'rest': 811,\n '15': 812,\n 'leaves': 813,\n 'inside': 814,\n 'sweetener': 815,\n 'leave': 816,\n 'expect': 817,\n 'course': 818,\n 'jelly': 819,\n 'future': 820,\n 'moist': 821,\n 'www': 822,\n 'stevia': 823,\n 'yogurt': 824,\n 'raspberry': 825,\n 'opinion': 826,\n 'addition': 827,\n 'sweetness': 828,\n 'lower': 829,\n 'recipes': 830,\n 'job': 831,\n 'party': 832,\n 'simple': 833,\n 'given': 834,\n 'stay': 835,\n 'http': 836,\n 'em': 837,\n 'packets': 838,\n 'description': 839,\n 'sell': 840,\n 'aren': 841,\n 'gp': 842,\n 'double': 843,\n 'none': 844,\n 'recently': 845,\n 'ship': 846,\n 'mother': 847,\n 'timely': 848,\n 'worst': 849,\n 'pot': 850,\n 'crispy': 851,\n 'gf': 852,\n 'hair': 853,\n 'href': 854,\n 'candies': 855,\n 'garlic': 856,\n 'mixes': 857,\n 'tiny': 858,\n 'experience': 859,\n 'impressed': 860,\n 'remember': 861,\n 'toy': 862,\n 'pleasant': 863,\n 'skin': 864,\n '9': 865,\n 'vet': 866,\n 'thick': 867,\n 'nasty': 868,\n 'crisp': 869,\n 'within': 870,\n 'stopped': 871,\n 'sea': 872,\n 'mess': 873,\n 'list': 874,\n 'terrific': 875,\n 'touch': 876,\n 'hours': 877,\n 'afternoon': 878,\n 'summer': 879,\n 'banana': 880,\n 'kid': 881,\n 'adds': 882,\n 'homemade': 883,\n 'loose': 884,\n 'five': 885,\n 'costco': 886,\n 'dressing': 887,\n 'forward': 888,\n 'maple': 889,\n 'become': 890,\n 'throw': 891,\n 'lime': 892,\n 'saw': 893,\n 'drinker': 894,\n 'advertised': 895,\n 'together': 896,\n '30': 897,\n 'tuna': 898,\n 'glass': 899,\n 'contains': 900,\n 'certainly': 901,\n 'supermarket': 902,\n 'hands': 903,\n 'roasted': 904,\n 'finish': 905,\n 'entire': 906,\n 'shape': 907,\n 'seen': 908,\n 'cappuccino': 909,\n 'done': 910,\n 'thin': 911,\n 'today': 912,\n 'serve': 913,\n 'quantity': 914,\n 'alot': 915,\n 'due': 916,\n 'return': 917,\n 'normally': 918,\n 'adding': 919,\n 'set': 920,\n 'paid': 921,\n 'source': 922,\n 'expiration': 923,\n 'bodied': 924,\n 'unique': 925,\n 'pancakes': 926,\n 'door': 927,\n 'outstanding': 928,\n 'ingredient': 929,\n 'birthday': 930,\n 'particular': 931,\n 'hoping': 932,\n 'warm': 933,\n 'convenience': 934,\n 'allergies': 935,\n 'creamer': 936,\n 'beautiful': 937,\n 'dish': 938,\n 'brewed': 939,\n 'enjoys': 940,\n 'helped': 941,\n 'wellness': 942,\n 'expecting': 943,\n 'dessert': 944,\n 'bring': 945,\n 'breath': 946,\n 'mustard': 947,\n 'plan': 948,\n 'uses': 949,\n 'carrying': 950,\n 'consistency': 951,\n 'ready': 952,\n 'described': 953,\n 'mango': 954,\n 'peanuts': 955,\n 'completely': 956,\n 'chocolates': 957,\n 'poor': 958,\n 'newman': 959,\n 'strawberry': 960,\n 'benefits': 961,\n 'ask': 962,\n 'picture': 963,\n 'overpriced': 964,\n 'microwave': 965,\n 'otherwise': 966,\n 'batch': 967,\n 'children': 968,\n 'based': 969,\n 'nutritious': 970,\n 'varieties': 971,\n 'except': 972,\n 'near': 973,\n 'veggies': 974,\n 'nicely': 975,\n 'fit': 976,\n 'herbal': 977,\n 'agree': 978,\n 'movie': 979,\n 'pork': 980,\n 'hate': 981,\n '11': 982,\n 'bowl': 983,\n 'under': 984,\n 'pill': 985,\n 'beware': 986,\n 'mini': 987,\n 'yourself': 988,\n 'normal': 989,\n 'six': 990,\n 'running': 991,\n 'promptly': 992,\n 'individual': 993,\n 'turned': 994,\n 'seemed': 995,\n 'bigger': 996,\n 'nearly': 997,\n 'disappointing': 998,\n 'replacement': 999,\n ...}"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "word2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
    }
   ],
   "source": [
    "# the embeddings from tf_hub. \n",
    "# embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "embed = hub.Module(\"https://tfhub.dev/google/Wiki-words-250/1\")\n",
    "emb = embed([key for key in word2ind.keys()])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    embedding_matrix = sess.run(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "np.shape(embedding_matrix)\n",
    "#embedding_matrix[0]\n",
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(57160, 250)\n"
    }
   ],
   "source": [
    "print(embedding_matrix.shape)\n",
    "np.save('./tf_hub_embedding.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts words in texts and summaries to indices\n",
    "# it looks like we have to set eos here to False\n",
    "converted_texts, unknown_words_in_texts = data_process.convert_text_to_indx(processed_texts, word2ind, eos=False, sos=False)\n",
    "\n",
    "converted_summaries, unknown_words_in_summaries = data_process.convert_text_to_indx(processed_summaries, word2ind, eos=True, sos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   counts\n0      48\n1      32\n2      41",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>counts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>41</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "create_lengths(converted_texts[:3])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Summaries:\n              counts\ncount  282345.000000\nmean        4.526537\nstd         2.229878\nmin         1.000000\n25%         3.000000\n50%         4.000000\n75%         6.000000\nmax        31.000000\n\nTexts:\n              counts\ncount  282345.000000\nmean       35.325765\nstd        11.547204\nmin         4.000000\n25%        26.000000\n50%        34.000000\n75%        44.000000\nmax        73.000000\n"
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(converted_summaries)\n",
    "lengths_texts = create_lengths(converted_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "52.0\n55.0\n60.0\n"
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 89.5))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7.0\n9.0\n12.0\n"
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "'<PAD>' has id: 2\npad summaries batch samples:\n [[  152     6  1073     1     2     2     2     2]\n [  960  4020   145     1     2     2     2     2]\n [  556    13  4020    44    85    24   817     1]\n [  958    39     1     2     2     2     2     2]\n [   28     8     1     2     2     2     2     2]\n [  342   525 16420     1     2     2     2     2]\n [  161   152     1     2     2     2     2     2]\n [   67    37     1     2     2     2     2     2]\n [ 4020     1     2     2     2     2     2     2]\n [  610   840    21    15  3178     1     2     2]\n [ 4020   960     1     2     2     2     2     2]\n [  868    66    41     1     2     2     2     2]\n [   16  1141    12     4    51     1     2     2]\n [    9    11    14    39     1     2     2     2]\n [   28   182   100   489     1     2     2     2]\n [    8    29   489     1     2     2     2     2]\n [   69    16     1     2     2     2     2     2]\n [   17   139   309     1     2     2     2     2]\n [   16    39     6   934     1     2     2     2]\n [ 1834   489     1     2     2     2     2     2]]\n"
    }
   ],
   "source": [
    "print(\"'<PAD>' has id: {}\".format(word2ind['<PAD>']))\n",
    "\n",
    "sorted_summaries_samples = converted_summaries[7:50]\n",
    "sorted_texts_samples = converted_texts[7:50]\n",
    "\n",
    "pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(data_process.get_batches(word2ind, sorted_summaries_samples, sorted_texts_samples, 20))\n",
    "\n",
    "print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "__pickleStuff(\"./data/embedding_matrix.p\", embedding_matrix)\n",
    "\n",
    "__pickleStuff(\"./data/converted_summaries.p\", converted_summaries)\n",
    "__pickleStuff(\"./data/converted_texts.p\", converted_texts)\n",
    "\n",
    "__pickleStuff(\"./data/word2ind.p\",word2ind)\n",
    "__pickleStuff(\"./data/ind2word.p\",ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing finished here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = __loadStuff(\"./data/embedding_matrix.p\")\n",
    "\n",
    "converted_summaries = __loadStuff(\"./data/converted_summaries.p\")\n",
    "converted_texts = __loadStuff(\"./data/converted_texts.p\")\n",
    "\n",
    "word2ind = __loadStuff(\"./data/word2ind.p\")\n",
    "ind2word = __loadStuff(\"./data/ind2word.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[17, 142, 71, 69, 1], [18, 35, 895, 1], [2801, 1717, 1]]"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "word2ind\n",
    "ind2word\n",
    "converted_summaries[:3]\n",
    "#converted_texts[:3]\n",
    "#word_embedding_matrix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\n",
    "\n",
    "\n",
    "\n",
    "def process_decoder_train_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<SOS>']), ending], 1)\n",
    "\n",
    "    return dec_input\n",
    "\n",
    "\n",
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            # original code is missing this line below, that is how we connect layers \n",
    "            # by feeding the current layer's output to next layer's input\n",
    "            rnn_inputs = enc_output\n",
    "    return enc_output, enc_state\n",
    "\n",
    "\n",
    "\n",
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer, vocab_size, max_summary_length, batch_size):\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n",
    "                                                       helper=training_helper,\n",
    "                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                       output_layer = output_layer)\n",
    "\n",
    "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits\n",
    "\n",
    "\n",
    "\n",
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer, max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits\n",
    "\n",
    "\n",
    "\n",
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                     enc_output,\n",
    "                                                     text_length,\n",
    "                                                     normalize=False,\n",
    "                                                     name='BahdanauAttention')\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size,\n",
    "                                                  max_summary_length,\n",
    "                                                  batch_size)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,\n",
    "                                                    vocab_to_int['<SOS>'],\n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell,\n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "    return training_logits, inference_logits\n",
    "\n",
    "\n",
    "\n",
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, word_embedding_matrix):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_decoder_train_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "\n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers,\n",
    "                                                        )\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.01\n",
    "keep_probability = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:From <ipython-input-47-9ee5f2ef4db3>:27: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:From <ipython-input-47-9ee5f2ef4db3>:27: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:From <ipython-input-47-9ee5f2ef4db3>:40: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\nWARNING:tensorflow:From <ipython-input-47-9ee5f2ef4db3>:40: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\nWARNING:tensorflow:From /Users/xinsun/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\nWARNING:tensorflow:From /Users/xinsun/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\nWARNING:tensorflow:From /Users/xinsun/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /Users/xinsun/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /Users/xinsun/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /Users/xinsun/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From <ipython-input-47-9ee5f2ef4db3>:91: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:From <ipython-input-47-9ee5f2ef4db3>:91: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:From <ipython-input-47-9ee5f2ef4db3>:98: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:From <ipython-input-47-9ee5f2ef4db3>:98: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:From /Users/xinsun/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /Users/xinsun/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nGraph is built.\n./graph\n"
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(word2ind)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers,\n",
    "                                                      word2ind, \n",
    "                                                      batch_size,\n",
    "                                                      word_embedding_matrix)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        \n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "282345"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "len(converted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The shortest text length: 4\nThe longest text length: 73\n"
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 100000\n",
    "end = start + 150000\n",
    "sorted_summaries_short = sorted(converted_summaries[start:end], key=lambda item: len(item))\n",
    "sorted_texts_short = sorted(converted_texts[start:end], key=lambda item: len(item))\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch   1/10 Batch  100/2343 - Loss:  3.722, Seconds: 127.54\nEpoch   1/10 Batch  200/2343 - Loss:  2.938, Seconds: 87.56\nEpoch   1/10 Batch  300/2343 - Loss:  2.897, Seconds: 141.60\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-894ed8d9280a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                  \u001b[0msummary_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                  \u001b[0mtext_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                  keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 100 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                data_process.get_batches(word2ind, sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['mary']"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(processed_texts) == len(converted_texts)\n",
    "assert len(processed_texts[5000]) == len(converted_texts[5000])\n",
    "\n",
    "converted_texts[5000]\n",
    "\n",
    "processed_texts[5000]\n",
    "processed_summaries[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "\n",
    "    return [word2ind.get(word, word2ind['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\nINFO:tensorflow:Restoring parameters from ./best_model.ckpt\n- Review:\n the flowers do not get as opened as they look on the picture and the tea does not taste that well very dissapointing\n- Summary:\n <SOS> zico hhhmmm hhhmmm <EOS>\n\n\n"
    }
   ],
   "source": [
    "'''\n",
    "input_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\", \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\n",
    "'''\n",
    "input_sentences = ['the flowers do not get as opened as they look on the picture and the tea does not taste that well very dissapointing']\n",
    "\n",
    "\n",
    "generagte_summary_length =  5\n",
    "\n",
    "texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "if type(generagte_summary_length) is list:\n",
    "    if len(input_sentences)!=len(generagte_summary_length):\n",
    "        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n",
    "    generagte_summary_length_list = generagte_summary_length\n",
    "else:\n",
    "    generagte_summary_length_list = [generagte_summary_length] * len(texts)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    for i, text in enumerate(texts):\n",
    "        generagte_summary_length = generagte_summary_length_list[i]\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0] \n",
    "        # Remove the padding from the summaries\n",
    "        pad = word2ind[\"<PAD>\"] \n",
    "        print('- Review:\\n\\r {}'.format(input_sentences[i]))\n",
    "        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([ind2word[i] for i in answer_logits if i != pad])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1360"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2ind['zico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The model\n",
    "\n",
    "Now we can build and train our model. First we define the hyperparameters we want to use. Then we create our Summarizer and call the function .build_graph(), which as the name suggests, builds the computation graph. \n",
    "Then we can train the model using .train()\n",
    "\n",
    "After training we can try our model using .infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "\n",
    "We can optionally use a cyclic learning rate, which we do here. \n",
    "I trained the model for 20 epochs and the loss was low then, but we could train it longer and would probably get better results.\n",
    "\n",
    "Unfortunately I do not have the resources to find the perfect (or right) hyperparameters, but these do pretty well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparametes\n",
    "num_layers_encoder = 4\n",
    "num_layers_decoder = 4\n",
    "rnn_size_encoder = 512\n",
    "rnn_size_decoder = 512\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "clip = 5\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.0005\n",
    "max_lr=0.005\n",
    "learning_rate_decay_steps = 700\n",
    "learning_rate_decay = 0.90\n",
    "\n",
    "\n",
    "pretrained_embeddings_path = './tf_hub_embedding.npy'\n",
    "summary_dir = os.path.join('./tensorboard', str('Nn_' + str(rnn_size_encoder) + '_Lr_' + str(learning_rate)))\n",
    "\n",
    "\n",
    "use_cyclic_lr = True\n",
    "inference_targets=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(converted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(78862*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph and train the model \n",
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   save_path='./models/amazon/my_model',\n",
    "                                   mode='TRAIN',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   batch_size = batch_size,\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = keep_probability,\n",
    "                                   learning_rate = learning_rate,\n",
    "                                   max_lr=max_lr,\n",
    "                                   learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                                   learning_rate_decay = learning_rate_decay,\n",
    "                                   epochs = epochs,\n",
    "                                   pretrained_embeddings_path = pretrained_embeddings_path,\n",
    "                                   use_cyclic_lr = use_cyclic_lr,\n",
    "                                   summary_dir = summary_dir)           \n",
    "\n",
    "summarizer.build_graph()\n",
    "summarizer.train(converted_texts[:70976], \n",
    "                 converted_summaries[:70976],\n",
    "                 validation_inputs=converted_texts[70976:],\n",
    "                 validation_targets=converted_summaries[70976:])\n",
    "\n",
    "\n",
    "# hidden training output.\n",
    "# both train and validation loss decrease nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inference\n",
    "Now we can use our trained model to create summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   './models/amazon/my_model',\n",
    "                                   'INFER',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   batch_size = len(converted_texts[:50]),\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = 1.0,\n",
    "                                   learning_rate = 0.0,\n",
    "                                   beam_width = 5,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   inference_targets = True,\n",
    "                                   pretrained_embeddings_path = pretrained_embeddings_path)\n",
    "\n",
    "summarizer.build_graph()\n",
    "preds = summarizer.infer(converted_texts[:50],\n",
    "                         restore_path =  './models/amazon/my_model',\n",
    "                         targets = converted_summaries[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "summarizer_model_utils.sample_results(preds,\n",
    "                                      ind2word,\n",
    "                                      word2ind,\n",
    "                                      converted_summaries[:50],\n",
    "                                      converted_texts[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b = a.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3, 4]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[4, 3, 2, 1]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}